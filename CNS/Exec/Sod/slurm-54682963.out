MPI initialized with 1 MPI processes
MPI initialized with thread support level 0
Initializing CUDA...
==PROF== Connected to process 2217500 (/home/ziangli/amrex/CNS/Exec/Sod/CNS3d.gnu.COMTR_PROF.MPI.CUDA.ex)
CUDA initialized with 1 device.
==PROF== Profiling "launch_global" - 0: 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 20 passes
AMReX (2b1434edf43e-dirty) initialized
Successfully read inputs file ... 
Successfully read inputs file ... 
Starting to call amrex_probinit ... 
Successfully run amrex_probinit
==PROF== Profiling "launch_global" - 1: 0%....50%....100% - 20 passes
==PROF== Profiling "launch_global" - 2: 0%....50%....100% - 20 passes
==PROF== Profiling "launch_global" - 3: 0%....50%....100% - 20 passes
==PROF== Profiling "launch_global" - 4: 0%....50%....100% - 20 passes
==PROF== Profiling "launch_global" - 5: 0%....50%....100% - 20 passes
==PROF== Profiling "launch_global" - 6: 0%....50%....100% - 20 passes
==PROF== Profiling "launch_global" - 7: 0%....50%....100% - 20 passes
==PROF== Profiling "launch_global" - 8: 0%
==ERROR== Failed to profile "launch_global" in process 2217500
==PROF== Trying to shutdown target application
==ERROR== The application returned an error code (9).
==ERROR== An error occurred while trying to profile.
[2217500] CNS3d.gnu.COMTR_PROF.MPI.CUDA.ex@127.0.0.1
  void amrex::launch_global<(int)256, std::enable_if<amrex::MaybeDeviceRunnable<T3, void>::value, void>::type amrex::ParallelFor<(int)256, int, <unnamed>::ResizeRandomSeed(unsigned long)::[lambda(int) (instance 1)], void>(const amrex::Gpu::KernelInfo &, T2, T3 &&)::[lambda() (instance 1)]>(T2) (2560, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 7.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       889.14
    SM Frequency            cycle/nsecond         1.27
    Elapsed Cycles                  cycle    7,981,050
    Memory Throughput                   %        64.83
    DRAM Throughput                     %         2.71
    Duration                      msecond         6.29
    L1/TEX Cache Throughput             %        68.14
    L2 Cache Throughput                 %        12.32
    SM Active Cycles                cycle 7,589,059.49
    Compute (SM) Throughput             %        61.93
    ----------------------- ------------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,560
    Registers Per Thread             register/thread              63
    Shared Memory Configuration Size            byte               0
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread         655,360
    Waves Per SM                                                   8
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        44.73
    Achieved Active Warps Per SM           warp        28.62
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of required registers. See the CUDA Best 
          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      
          details on optimizing occupancy.                                                                              

  void amrex::launch_global<(int)256, std::enable_if<amrex::MaybeDeviceRunnable<T2, void>::value, void>::type amrex::ParallelFor<(int)256, CNS::initData()::[lambda(int, int, int) (instance 1)]>(const amrex::Gpu::KernelInfo &, const amrex::Box &, T2 &&)::[lambda() (instance 1)]>(T2) (128, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 7.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       815.95
    SM Frequency            cycle/nsecond         1.15
    Elapsed Cycles                  cycle        6,008
    Memory Throughput                   %        22.45
    DRAM Throughput                     %         6.71
    Duration                      usecond         5.22
    L1/TEX Cache Throughput             %        33.05
    L2 Cache Throughput                 %        22.45
    SM Active Cycles                cycle     3,545.57
    Compute (SM) Throughput             %         9.29
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size            byte               0
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          32,768
    Waves Per SM                                                0.27
    -------------------------------- --------------- ---------------

    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %        18.31
    Achieved Active Warps Per SM           warp        11.72
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers. The difference    
          between calculated theoretical (75.0%) and measured achieved occupancy (18.3%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void amrex::launch_global<(int)256, std::enable_if<amrex::MaybeDeviceRunnable<T2, void>::value, void>::type amrex::ParallelFor<(int)256, CNS::initData()::[lambda(int, int, int) (instance 1)]>(const amrex::Gpu::KernelInfo &, const amrex::Box &, T2 &&)::[lambda() (instance 1)]>(T2) (128, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 7.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       788.92
    SM Frequency            cycle/nsecond         1.12
    Elapsed Cycles                  cycle        5,980
    Memory Throughput                   %        22.52
    DRAM Throughput                     %         6.52
    Duration                      usecond         5.34
    L1/TEX Cache Throughput             %        33.20
    L2 Cache Throughput                 %        22.52
    SM Active Cycles                cycle     3,593.61
    Compute (SM) Throughput             %         9.33
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size            byte               0
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          32,768
    Waves Per SM                                                0.27
    -------------------------------- --------------- ---------------

    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %        18.48
    Achieved Active Warps Per SM           warp        11.83
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers. The difference    
          between calculated theoretical (75.0%) and measured achieved occupancy (18.5%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void amrex::launch_global<(int)256, std::enable_if<amrex::MaybeDeviceRunnable<T2, void>::value, void>::type amrex::ParallelFor<(int)256, CNS::initData()::[lambda(int, int, int) (instance 1)]>(const amrex::Gpu::KernelInfo &, const amrex::Box &, T2 &&)::[lambda() (instance 1)]>(T2) (128, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 7.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       770.11
    SM Frequency            cycle/nsecond         1.09
    Elapsed Cycles                  cycle        6,074
    Memory Throughput                   %        22.18
    DRAM Throughput                     %         6.37
    Duration                      usecond         5.57
    L1/TEX Cache Throughput             %        32.67
    L2 Cache Throughput                 %        22.18
    SM Active Cycles                cycle     3,651.74
    Compute (SM) Throughput             %         9.18
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size            byte               0
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          32,768
    Waves Per SM                                                0.27
    -------------------------------- --------------- ---------------

    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %        18.29
    Achieved Active Warps Per SM           warp        11.71
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers. The difference    
          between calculated theoretical (75.0%) and measured achieved occupancy (18.3%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void amrex::launch_global<(int)256, std::enable_if<amrex::MaybeDeviceRunnable<T2, void>::value, void>::type amrex::ParallelFor<(int)256, CNS::initData()::[lambda(int, int, int) (instance 1)]>(const amrex::Gpu::KernelInfo &, const amrex::Box &, T2 &&)::[lambda() (instance 1)]>(T2) (128, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 7.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       792.17
    SM Frequency            cycle/nsecond         1.12
    Elapsed Cycles                  cycle        5,964
    Memory Throughput                   %        22.59
    DRAM Throughput                     %         6.55
    Duration                      usecond         5.31
    L1/TEX Cache Throughput             %        33.27
    L2 Cache Throughput                 %        22.59
    SM Active Cycles                cycle     3,563.97
    Compute (SM) Throughput             %         9.35
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size            byte               0
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          32,768
    Waves Per SM                                                0.27
    -------------------------------- --------------- ---------------

    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %        18.43
    Achieved Active Warps Per SM           warp        11.79
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers. The difference    
          between calculated theoretical (75.0%) and measured achieved occupancy (18.4%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void amrex::launch_global<(int)256, std::enable_if<amrex::MaybeDeviceRunnable<T2, void>::value, void>::type amrex::ParallelFor<(int)256, CNS::initData()::[lambda(int, int, int) (instance 1)]>(const amrex::Gpu::KernelInfo &, const amrex::Box &, T2 &&)::[lambda() (instance 1)]>(T2) (128, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 7.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       790.42
    SM Frequency            cycle/nsecond         1.12
    Elapsed Cycles                  cycle        5,988
    Memory Throughput                   %        22.48
    DRAM Throughput                     %         6.71
    Duration                      usecond         5.34
    L1/TEX Cache Throughput             %        33.15
    L2 Cache Throughput                 %        22.48
    SM Active Cycles                cycle     3,591.80
    Compute (SM) Throughput             %         9.32
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size            byte               0
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          32,768
    Waves Per SM                                                0.27
    -------------------------------- --------------- ---------------

    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %        18.37
    Achieved Active Warps Per SM           warp        11.76
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers. The difference    
          between calculated theoretical (75.0%) and measured achieved occupancy (18.4%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void amrex::launch_global<(int)256, std::enable_if<amrex::MaybeDeviceRunnable<T2, void>::value, void>::type amrex::ParallelFor<(int)256, CNS::initData()::[lambda(int, int, int) (instance 1)]>(const amrex::Gpu::KernelInfo &, const amrex::Box &, T2 &&)::[lambda() (instance 1)]>(T2) (128, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 7.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       793.41
    SM Frequency            cycle/nsecond         1.12
    Elapsed Cycles                  cycle        5,998
    Memory Throughput                   %        22.46
    DRAM Throughput                     %         6.73
    Duration                      usecond         5.34
    L1/TEX Cache Throughput             %        33.09
    L2 Cache Throughput                 %        22.46
    SM Active Cycles                cycle     3,643.86
    Compute (SM) Throughput             %         9.30
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size            byte               0
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          32,768
    Waves Per SM                                                0.27
    -------------------------------- --------------- ---------------

    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %        18.42
    Achieved Active Warps Per SM           warp        11.79
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers. The difference    
          between calculated theoretical (75.0%) and measured achieved occupancy (18.4%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void amrex::launch_global<(int)256, std::enable_if<amrex::MaybeDeviceRunnable<T2, void>::value, void>::type amrex::ParallelFor<(int)256, CNS::initData()::[lambda(int, int, int) (instance 1)]>(const amrex::Gpu::KernelInfo &, const amrex::Box &, T2 &&)::[lambda() (instance 1)]>(T2) (128, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 7.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       803.57
    SM Frequency            cycle/nsecond         1.13
    Elapsed Cycles                  cycle        6,103
    Memory Throughput                   %        22.09
    DRAM Throughput                     %         6.84
    Duration                      usecond         5.38
    L1/TEX Cache Throughput             %        32.54
    L2 Cache Throughput                 %        22.09
    SM Active Cycles                cycle     3,748.69
    Compute (SM) Throughput             %         9.14
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size            byte               0
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          32,768
    Waves Per SM                                                0.27
    -------------------------------- --------------- ---------------

    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %        18.50
    Achieved Active Warps Per SM           warp        11.84
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers. The difference    
          between calculated theoretical (75.0%) and measured achieved occupancy (18.5%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[22079,1],0]
  Exit code:    9
--------------------------------------------------------------------------
slurmstepd: error: Detected 1 oom_kill event in StepId=54682963.batch. Some of the step tasks have been OOM Killed.
